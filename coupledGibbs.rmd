---
title: "Stat 248 Project Gibbs"
output: pdf_document
---


By MVN conditionals, we have that for MVN $(\bf{Y_1}, \bf{Y_2}) \sim \mathcal N(\bf{\mu}, V)$ where $\mu$ is the vector of expectations such that $E[\textbf{Y}_1] = \mu_1$ and $E[\textbf{Y}_2]=\mu_2$, and $V$ is the matrix of covariances such that $V = \begin{pmatrix}V_{11} & V_{12} \\ V_{21} & V_{22}\end{pmatrix},$ we can write the conditional distribution as
$$\textbf{Y}_2|\textbf{Y}_1\sim \mathcal N\left(\mu_2+V_{21}V_{11}^{-1}(Y_1 -\mu_1),V_{22}-V_{21}V^{-1}_{11}V_{12}\right).$$

For this Gibbs sampler, we have that $\textbf{Y}_2$ is the element that we are currently updating. We will choose a symmetric covariance matrix $V$ such that the covariance between any pair of normals is the same, and the variance of any normal is also the same. Hence $V_{22}, V_{21}, V_{12}, V_{11}$ stay the same from term to term that we are updating.

Say that our starting distribution $p_0$ is a series of iid normals, and we would like to sample with covariances as given.

```{r}
# initializing variables
library(MASS)

n = 4
N = 100

cov = 0.5
cov.mat = matrix(rep(cov,n^2), nrow = n)
for(i in 1:n){cov.mat[i,i] = 1}

ind.mat = matrix(rep(0,n^2), nrow = n)
for(i in 1:n){ind.mat[i,i] = 1}

yt = xt = matrix(rep(0, n*(N+1)), nrow = N+1)
```

```{r}
# helper function to simulate mvn gibbs sampler
gibbs.mvn = function(xt, cov.mat){
  v22 = cov.mat[1,1]
  v11 = cov.mat[2:n,2:n]
  v12 = cov.mat[1,2:n]
  sd = sqrt(v22 - v12 %*% solve(v11) %*% v12)
  
  n = dim(xt)[2]
  N = dim(xt)[1]-1
  
  for(i in 2:(N+1)){ # i is the index of the current row
    for(j in 1:n){ # j is the index of the current column
      past = xt[i,1:(j-1)]
      if(j < n){
        past = tail(c(past, xt[i-1,(j+1):n]),n-1)
      }
      mu = v12 %*% solve(v11) %*% past
      xt[i,j] = qnorm(p=xt[i,j], mean=mu, sd=sd)
    }
  }
  return(xt)
}


# initial run of gibbs
set.seed(248)
xt[2:(N+1),] = matrix(runif(n*N), nrow = N)
mu = 0.5
xt[1,] = mvrnorm(mu = rep(mu,n), Sigma= ind.mat)
xt = gibbs.mvn(xt, cov.mat)


# wrap-around gibbs sampled chain
set.seed(248)
yt[2:(N+1),] = matrix(runif(n*N), nrow = N)
yt[1,] = xt[N+1,]
yt = gibbs.mvn(yt, cov.mat)
```

```{r}
# plot convergence of each coupled chain
par(mfrow=c(2,2))
plot(xt[,1]~c(0:N), type="l", lwd=2, xlab="Time", ylab = "Sampled Value", main="1st Coordinate")
lines(yt[,1]~c(0:N), col="grey")
plot(xt[,2]~c(0:N), type="l", lwd=2, xlab="Time", ylab = "Sampled Value", main="2nd Coordinate")
lines(yt[,2]~c(0:N), col="grey")
plot(xt[,3]~c(0:N), type="l", lwd=2, xlab="Time", ylab = "Sampled Value", main="3rd Coordinate")
lines(yt[,3]~c(0:N), col="grey")
plot(xt[,4]~c(0:N), type="l", lwd=2, xlab="Time", ylab = "Sampled Value", main="4th Coordinate")
lines(yt[,4]~c(0:N), col="grey")
```

We can see high rates of convergence before $N=50$ terms!

```{r}
# plot log squared euclidean distances over time
dist = log(rowSums((yt-xt)^2))
plot(dist~c(0:N), type="l", ylab = "Log of Squared Euclidean Distance", xlab="Time")

# plot against normal densities
hist(yt, breaks = 20, freq=FALSE, main="Histogram of Wraparound Chain vs. Normal PDF")
curve(dnorm, add = TRUE, col="red")
```



```{r}
# auxiliary chains

r = 5 # number of auxiliary chains
start = matrix(rep(NA, n*(N+1)), ncol = n)
chain = matrix(rep(NA, n*(N+1)*r), ncol = n)

set.seed(248)
unifs = matrix(runif(n*N), nrow = N)

wrap = function(mat,brk){
  x = mat
  N = nrow(mat)
  x[1:(N - brk+1),] = mat[brk:N,]
  x[(N-brk + 2):N,] = mat[1:brk-1,]
  return(x)
}

set.seed(123)
for(i in 1:(r-1)){
  s = i * N/r
  start[1,] = mvrnorm(mu = rep(mu,n), Sigma= ind.mat)
  start[2:(N+1),] = wrap(unifs, s)
  # start[2:(N-s+1),] = unifs[s:N,]
  # start[(N-s+2):(N+1),] = unifs[1:s-1]
  chain[((i-1)*(N+1) + 1):(i*(N+1)),] = wrap(gibbs.mvn(start,cov.mat), N-s+3)
}
```
```{r}
par(mfrow=c(2,2))

plot(xt[,1]~c(0:N), type="l", lwd=2, xlab="Time", ylab = "Sampled Value", 
     main="1st Coordinate", ylim = c(-2.5,2.2))
lines(yt[,1]~c(0:N), col="grey")
for(i in 1:(r-1)){
  s = i * N/r
  points(chain[(i-1) * (N+1) + s+1,1]~c(s))
  lines(chain[((i-1) * (N+1) + s+1):(i * (N+1)),1]~c(s:N), col=i+1)
}

plot(xt[,2]~c(0:N), type="l", lwd=2, xlab="Time", ylab = "Sampled Value", 
     main="2nd Coordinate")
lines(yt[,2]~c(0:N), col="grey")
for(i in 1:(r-1)){
  s = i * N/r
  points(chain[(i-1) * (N+1) + s+1,2]~c(s))
  lines(chain[((i-1) * (N+1) + s+1):(i * (N+1)),2]~c(s:N), col=i+1)
}

plot(xt[,3]~c(0:N), type="l", lwd=2, xlab="Time", ylab = "Sampled Value", 
     main="3rd Coordinate")
lines(yt[,3]~c(0:N), col="grey")
for(i in 1:(r-1)){
  s = i * N/r
  points(chain[(i-1) * (N+1) + s+1,3]~c(s))
  lines(chain[((i-1) * (N+1) + s+1):(i * (N+1)),3]~c(s:N), col=i+1)
}

plot(xt[,4]~c(0:N), type="l", lwd=2, xlab="Time", ylab = "Sampled Value", 
     main="4th Coordinate")
lines(yt[,4]~c(0:N), col="grey")
for(i in 1:(r-1)){
  s = i * N/r
  points(chain[(i-1) * (N+1) + s+1,4]~c(s))
  lines(chain[((i-1) * (N+1) + s+1):(i * (N+1)),4]~c(s:N), col=i+1)
}
```